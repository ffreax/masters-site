<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
	"http://www.w3.org/TR/html4/loose.dtd">
<html>
<head><meta http-equiv="content-type" content="text/html; charset=UTF-8"/><meta name="generator" content="ABBYY FineReader 11"/><meta name="author" content="xoyo"/><meta name="description" content="gnuplot plot"/><title>cache_iter.eps</title><link rel="stylesheet" href="hierarchical-ostrich_files/hierarchical-ostrich.css" type="text/css"/>
</head>
<body>
<p><a name="bookmark0"></a><span class="font4" style="font-weight:bold;">A Hierarchical Approach to Maximizing MapReduce Efficiency</span></p>
<p><span class="font3">Zhiwei Xiao, Haibo Chen, Binyu Zang</span></p>
<p><span class="font3" style="font-style:italic;">Parallel Processing Institute, Fudan University {zwxiao, hbchen, byzang}@ftidan.edu.cn</span></p>
<p><span class="font3">II. </span><span class="font0" style="font-variant:small-caps;">Azwraith Design</span></p><div>
<p><span class="font0" style="font-variant:small-caps;">I. Introduction</span></p></div><br clear="all"/>
<p><span class="font3">MapReduce [1] has been widely recognized for its elastic scalability and fault tolerance, with the efficiency being relatively disregarded, which, however, is equally important in “pay-as-you-go” cloud systems such as Amazon’s Elastic MapReduce. This paper argues that there are multiple levels of data locality and parallelism in typical multicore clusters that could affect performance.</span></p>
<p><span class="font3">By characterizing the performance limitation of typical MapReduce applications on multi-core based Hadoop clusters, we show that current JVM-based runtime (i.e., TaskWorker) fails to exploit data locality and task parallelism at single-node level.</span></p>
<p><span class="font3">Specifically, the open-source implementation of MapReduce, Hadoop [2], employs a JVM runtime to run the actual MapReduce tasks, which is suboptimal to explore the cache hierarchy and task parallelism existing in many multi-core based commodity clusters. Hadoop requires both key and value objects to implement the Hadoop Writable interface to support serialization and deserialization, causing extra objects creation and destroy overhead as well as memory footprint.</span></p>
<p><span class="font3">Moreover, some applications require processing the same piece of data multiple times or iteratively to get the final results. Though Hadoop exploits data locality with a single iteration of jobs by moving computation to its data as much as possible, unfortunately, it does not consider data locality across multiple processing iterations, and thus requires the same data being loaded multiple times from the networking file systems to nodes that process the data.</span></p>
<p><span class="font3">Based on the above observations, we propose Azwraith, a hierarchical MapReduce approach aiming to maximize data locality and task parallelism of MapReduce applications on Hadoop. In the hierarchical MapReduce model of Azwraith, each Map or Reduce task assigned to a single node is treated as a separate MapReduce job and is further decomposed into a Map and a Reduce tasks, which are processed by a MapReduce runtime specially optimized on a single node. Specifically, Azwraith integrates an efficient MapReduce runtime (namely Ostrich [3]) for multi-core to Hadoop. To exploit data locality among nodes at networking level, Azwraith integrates an in-memory cache system that caches data in memory that will likely be reused again, to avoid unnecessary networking and disk traffics.</span></p>
<p><span class="font3">Instead of writing a new runtime from scratch, we reuse and adapt an efficient MapReduce implementation for shared memory multiprocessor to Hadoop, called Ostrich. Ostrich adopts the “tiling strategy” to MapReduce on multicore and aggressively exploits task parallelism and data locality on multicore. To minimize complexity, Azwraith follows exactly the workflow of Hadoop and leaves most components (e.g., TaskTracker, HDFS) untouched. Hadoop with the Azwraith extension can still run original Java-based jobs in the normal way.</span></p><img src="hierarchical-ostrich_files/hierarchical-ostrich-1.jpg" style="width:240pt;height:171pt;"/>
<p><span class="font0">Figure 1. Azwraith architecture: the solid (colored) components are that need to be adjusted for Azwraith: Azwraith replaces the TaskWorker with Ostrich that exploits the data locality and parallelism in multi-core; The RPC Client makes the </span><span class="font0" style="font-style:italic;">TaskWorker</span><span class="font0"> conform the Hadoop RPC protocol and provides RPC services to the </span><span class="font0" style="font-style:italic;">TaskWorker',</span><span class="font0"> The DFS Client provides HDFS accesses for the </span><span class="font0" style="font-style:italic;">TaskWorker.</span><span class="font0"> User can submit an Azwraith job or an original Hadoop job to the MapReduce system with JobClient.</span></p>
<p><span class="font3">Figure 1 shows the overall architecture of Azwraith, which resembles the original Hadoop. The execution of an Azwraith job also follows with the workflow of Hadoop. The client must specify the job type as Azwraith or Hadoop before submitting a job. The </span><span class="font3" style="font-style:italic;">TaskTracker</span><span class="font3"> can fork a process (in case of an Azwraith job) or start a JVM instance (in case of a Hadoop job) to run the assigned task, according to the job type.</span></p>
<p><span class="font3">To adapt the Ostrich runtime as the </span><span class="font3" style="font-style:italic;">TaskWorker</span><span class="font3"> of Hadoop, we modify Ostrich to conform to the workflow and communication protocols of Hadoop, including status reporting and output format. To communicate with the</span></p>
<p><span class="font3" style="font-style:italic;">TaskTracker,</span><span class="font3"> the new </span><span class="font3" style="font-style:italic;">TaskWorker</span><span class="font3"> is embedded with an RPC (Remote Procedure Call) client. The RPC client is used to make the new </span><span class="font3" style="font-style:italic;">TaskWorker</span><span class="font3"> conform to the Hadoop RPC protocol and provide RPC services to the </span><span class="font3" style="font-style:italic;">TaskWorker</span><span class="font3">. The </span><span class="font3" style="font-style:italic;">TaskWorker</span><span class="font3"> accesses the HDFS (Hadoop Distributed File System) with the DFS client module, which directs accesses to HDFS.</span></p>
<p><span class="font3">We further enhance the Azwraith runtime with sevral optimizations. First, Azwraith overlaps the CPU burst and the I/O burst to hide the I/O blocking time as much as possible. Second, Azwraith can aggressively reuse the data in memory with pointer operations and thus avoid copying large amount of memory and enjoy a good cache locality. Thirdly, Azwraith requires applications to implement a set of aggregative (de)serialization interfaces to (de)serialize a set of data together, which saves tremendous amount of function calls and gain a better data-locality than Hadoop.</span></p><div><img src="hierarchical-ostrich_files/hierarchical-ostrich-2.png" style="width:256pt;height:114pt;"/>
<p><span class="font1">Figure 2. Overall performance and the breakdown of WordCount, LinearRegression and GigaSort. The symbol of A-x refers to Azwraith with xGB input, while H-x refers to Hadoop with xGB input.</span></p></div><br clear="all"/><div><img src="hierarchical-ostrich_files/hierarchical-ostrich-3.png" style="width:218pt;height:149pt;"/>
<p><span class="font1">Figure 3. Performance of Azwraith Cache System on K-Means</span></p></div><br clear="all"/>
<p><span class="font3">To enable data reuse among tasks, we design and implement a cache server as a daemon process on each slave to serve all HDFS read accesses. The cache server leverages the shared memory interfaces and the semaphore mechanism in an operating system to provide control and data information to cache clients (i.e., </span><span class="font3" style="font-style:italic;">TaskWorkers),</span><span class="font3"> which access the shared memory managed by the cache server in the way of accessing the local memory. Using shared memory also gains a memory usage benefit, since all tasks in the same node can share one single copy of data and thus leave more memory available for caching and computing.</span></p>
<p><span class="font3">We also extend the affinity support of the Hadoop schedule, to schedule tasks to nodes where input data are cached. Azwraith maintains the cache locations information, with a mapping similar to Hadoop’s disk-local task mapping. When receiving a task assignment request from a slave (node), the scheduler would first get the map-tasks list for the requesting slave (node), and assign a cache-local task if any. Otherwise, the scheduler works as usual. The extension to the scheduling system makes around 50 lines of code changes to Hadoop.</span></p>
<p><span class="font3">III. </span><span class="font1" style="font-variant:small-caps;">Evaluation</span></p>
<p><span class="font3">We conducted the experiments on a small-scale cluster with 1 master node and 6 slave nodes. Each machine was equipped with two AMD Opteron 12-core processors, 64 GB main memory and 4 SCSI hard drives. Each machine connected to the same switch through a 1Gb Ethernet link. We used Hadoop version 0.20.1 running on Java SE Runtime 1.6.0. Azwraith was also built on the same version of Hadoop.</span></p>
<p><span class="font3">As shown in Figure 2, Azwraith gains a considerable speedup over Hadoop with different input sizes, ranging from 1.4x to 3.5x. Computation-oriented tasks like Word-Count and LinearRegression gain larger speedup than the I/O-intensive applications such as GigaSort.</span></p>
<p><span class="font3">As shown in Figure 3, with the support of the cache system, Azwraith gains a 1.43X to 1.55X speedup over Azwraith without the cache scheme, and 2.06X to 2.21X over Hadoop.</span></p>
<p><span class="font3">IV. </span><span class="font2" style="font-variant:small-caps;">Conclusion</span></p>
<p><span class="font3">In this paper, we argued that Hadoop has limitations in exploiting data locality and task parallelism for multi-core platforms. We then extended Hadoop with a hierarchical MapReduce scheme. An in-memory cache scheme is also seamlessly integrated to cache data that is likely to be accessed in memory. Evaluation showed that the hierarchical scheme outperforms Hadoop ranging from 1.4x to 3.5x.</span></p>
<p><span class="font2" style="font-variant:small-caps;">References</span></p>
<p><span class="font2">[1] &nbsp;&nbsp;&nbsp;J. Dean and S. Ghemawat, “MapReduce: simplified data processing on large clusters,” </span><span class="font2" style="font-style:italic;">Communications of the ACM, </span><span class="font2">vol. 51, no. 1, pp. 107-113, 2008.</span></p>
<p><span class="font2">[2] &nbsp;&nbsp;&nbsp;A. Bialecki, M. Cafarella, D. Cutting, and O. OMalley, “Hadoop: a framework for running applications on large clusters built of commodity hardware,” <a href="http://lucene">http://lucene</a>. apache. org/hadoop, 2005.</span></p>
<p><span class="font2">[3] &nbsp;&nbsp;&nbsp;R. Chen, H. Chen, and B. Zang, “Tiled mapreduce: Optimizing resource usages of data-parallel applications on multicore with tiling,” in </span><span class="font2" style="font-style:italic;">Proc. PACT,</span><span class="font2"> 2010.</span></p>
</body>
</html>